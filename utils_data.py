import pickle as pkl
from datetime import datetime
import pandas as pd
from sklearn.model_selection import train_test_split  # for splitting the data into train and test samples
from sklearn.preprocessing import MinMaxScaler  # for feature scaling
from sklearn.preprocessing import OrdinalEncoder  # to encode categorical variables
from matplotlib.pyplot import savefig, subplots
# from pandas_profiling import ProfileReport


def plot_results(trials, task, algo, measure):
    f, ax = subplots(1)  # , figsize=(10,10))
    xs = [t['misc']['vals']['n_neighbors'] for t in trials.trials]
    ys = [-t['result']['loss'] for t in trials.trials]
    ax.scatter(xs, ys, s=20, linewidth=0.01, alpha=0.5)
    ax.set_title( str(task) + ' - ' + str(algo), fontsize=18)
    ax.set_xlabel('n_neighbors', fontsize=12)
    ax.set_ylabel('cross validation accuracy', fontsize=12)
    print('saving plot...')
    savefig('training_loss_{}_{}_{}.png'.format(task, algo, measure))


# test function that evaluates the model on the test data and return the score
def test(model, test_data, test_label):
    # Make a prediction using the optimized model
    prediction = model.predict( test_data )
    # Report the accuracy of the classifier on a given set of data
    score = model.score( test_data, test_label )
    return score


def transform(df, aps):
    # Do Min-Max scaling
    scaler = MinMaxScaler()
    # fit transform all the AP columns
    df[aps] = df[aps].fillna(100)
    df['coord_z'] = df['coord_z'].fillna(100)
    df[aps] = scaler.fit_transform(df[aps])
    df['coord_x'] = scaler.fit_transform(df[['coord_x']])
    df['coord_y'] = scaler.fit_transform(df[['coord_y']])
    df['coord_z'] = scaler.fit_transform(df[['coord_z']])
    enc = OrdinalEncoder()  # select the encoder
    df['building'] = enc.fit_transform(df[['building']])  # encode categorical values
    df['floor'] = enc.fit_transform(df[['floor']])
    df['tile'] = enc.fit_transform(df[['tile']])
    # drop the rows with missing values
    df = df.dropna()
    return df


# save function save the model
def save(model, task, algorithm, distance):
    # Save the model
    print('Saving model...')
    # timestamp datetime
    date = datetime.now()
    ts = date.timestamp()
    ts = str(ts).split('.')[0]
    name_model_f = 'model_{}_{}_{}_{}'.format(task, algorithm, distance, ts)
    with open(name_model_f+'.pkl', 'wb') as f:
        pkl.dump(model, f)
    f.close()
    return name_model_f


# load function load the model
def load(model_path):
    # Load the model
    with open(model_path, 'rb') as f:
        model = pkl.load(f)
    f.close()
    return model


def preprocess(df, aps):
    # min-max scaling and ordinal encoding data
    df = transform(df, aps)
    # split
    df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)
    return df_train, df_test


# # funtion create_report use pandas-profiling to create reduced report and save it as html7
# def create_report(df):
#     report = ProfileReport(df,
#                            title='Your Autogenerated Prediction Report', html={'style':{'full_width':True}},
#                            explorative=True,
#                            minimal=True
#                            )
#
#     report.to_file("report.html")

# this function save csv results of the dataframes
def save_csv(final_res, metrics_df):
    final_res.to_csv('final_results.csv')
    metrics_df.to_csv('metrics.csv')


def save_excel(final_res, metrics_df):
    with pd.ExcelWriter('final_results.xlsx') as writer:
        final_res.to_excel(writer, sheet_name='final_results')
        metrics_df.to_excel(writer, sheet_name='metrics')
